{
  "name": "PDF to Questions Generator",
  "repoURLOrShorthand": "mastra-ai/template-pdf-questions",
  "videoURL": "https://youtu.be/WQ0rvX8ajeg",
  "description": "# PDF to Questions Generator\n\nA Mastra template that demonstrates **how to protect against token limits** by generating AI summaries from large datasets before passing as output from tool calls.\n\n> **üéØ Key Learning**: This template shows how to use large context window models (OpenAI GPT-4.1 Mini) as a \"summarization layer\" to compress large documents into focused summaries, enabling efficient downstream processing without hitting token limits.\n\n## Overview\n\nThis template showcases a crucial architectural pattern for working with large documents and LLMs:\n\n**üö® The Problem**: Large PDFs can contain 50,000+ tokens, which would overwhelm context windows and cost thousands of tokens for processing.\n\n**‚úÖ The Solution**: Use a large context window model (OpenAI GPT-4.1 Mini) to generate focused summaries, then use those summaries for downstream processing.\n\n### Workflow\n\n1. **Input**: PDF URL\n2. **Download & Summarize**: Fetch PDF, extract text, and generate AI summary using OpenAI GPT-4.1 Mini\n3. **Generate Questions**: Create focused questions from the summary (not the full text)\n\n### Key Benefits\n\n- **üìâ Token Reduction**: 80-95% reduction in token usage\n- **üéØ Better Quality**: More focused questions from key insights\n- **üí∞ Cost Savings**: Dramatically reduced processing costs\n- **‚ö° Faster Processing**: Summaries are much faster to process than full text\n\n## Prerequisites\n\n- Node.js 20.9.0 or higher\n- OpenAI API key (for both summarization and question generation)\n\n## Setup\n\n1. **Clone and install dependencies:**\n\n   ```bash\n   git clone <repository-url>\n   cd template-pdf-questions\n   pnpm install\n   ```\n\n2. **Set up environment variables:**\n\n   ```bash\n   cp .env.example .env\n   # Edit .env and add your API keys\n   ```\n\n   ```env\n   OPENAI_API_KEY=\"your-openai-api-key-here\"\n   ```\n\n3. **Run the example:**\n\n   ```bash\n   npx tsx example.ts\n   ```\n\n## üèóÔ∏è Architectural Pattern: Token Limit Protection\n\nThis template demonstrates a crucial pattern for working with large datasets in LLM applications:\n\n### The Challenge\n\nWhen processing large documents (PDFs, reports, transcripts), you often encounter:\n\n- **Token limits**: Documents can exceed context windows\n- **High costs**: Processing 50,000+ tokens repeatedly is expensive\n- **Poor quality**: LLMs perform worse on extremely long inputs\n- **Slow processing**: Large inputs take longer to process\n\n### The Solution: Summarization Layer\n\nInstead of passing raw data through your pipeline:\n\n1. **Use a large context window model** (OpenAI GPT-4.1 Mini) to digest the full content\n2. **Generate focused summaries** that capture key information\n3. **Pass summaries to downstream processing** instead of raw data\n\n### Implementation Details\n\n```typescript\n// ‚ùå BAD: Pass full text through pipeline\nconst questions = await generateQuestions(fullPdfText); // 50,000 tokens!\n\n// ‚úÖ GOOD: Summarize first, then process\nconst summary = await summarizeWithGPT41Mini(fullPdfText); // 2,000 tokens\nconst questions = await generateQuestions(summary); // Much better!\n```\n\n### When to Use This Pattern\n\n- **Large documents**: PDFs, reports, transcripts\n- **Batch processing**: Multiple documents\n- **Cost optimization**: Reduce token usage\n- **Quality improvement**: More focused processing\n- **Chain operations**: Multiple LLM calls on same data\n\n## Usage\n\n### Using the Workflow\n\n```typescript\nimport { mastra } from './src/mastra/index';\n\nconst run = await mastra.getWorkflow('pdfToQuestionsWorkflow').createRunAsync();\n\n// Using a PDF URL\nconst result = await run.start({\n  inputData: {\n    pdfUrl: 'https://example.com/document.pdf',\n  },\n});\n\nconsole.log(result.result.questions);\n```\n\n### Using the PDF Questions Agent\n\n```typescript\nimport { mastra } from './src/mastra/index';\n\nconst agent = mastra.getAgent('pdfQuestionsAgent');\n\n// The agent can handle the full process with natural language\nconst response = await agent.stream([\n  {\n    role: 'user',\n    content: 'Please download this PDF and generate questions from it: https://example.com/document.pdf',\n  },\n]);\n\nfor await (const chunk of response.textStream) {\n  console.log(chunk);\n}\n```\n\n### Using Individual Tools\n\n```typescript\nimport { mastra } from './src/mastra/index';\nimport { pdfFetcherTool } from './src/mastra/tools/download-pdf-tool';\nimport { generateQuestionsFromTextTool } from './src/mastra/tools/generate-questions-from-text-tool';\n\n// Step 1: Download PDF and generate summary\nconst pdfResult = await pdfFetcherTool.execute({\n  context: { pdfUrl: 'https://example.com/document.pdf' },\n  mastra,\n  runtimeContext: new RuntimeContext(),\n});\n\nconsole.log(`Downloaded ${pdfResult.fileSize} bytes from ${pdfResult.pagesCount} pages`);\nconsole.log(`Generated ${pdfResult.summary.length} character summary`);\n\n// Step 2: Generate questions from summary\nconst questionsResult = await generateQuestionsFromTextTool.execute({\n  context: {\n    extractedText: pdfResult.summary,\n    maxQuestions: 10,\n  },\n  mastra,\n  runtimeContext: new RuntimeContext(),\n});\n\nconsole.log(questionsResult.questions);\n```\n\n### Expected Output\n\n```javascript\n{\n  status: 'success',\n  result: {\n    questions: [\n      \"What is the main objective of the research presented in this paper?\",\n      \"Which methodology was used to collect the data?\",\n      \"What are the key findings of the study?\",\n      // ... more questions\n    ],\n    success: true\n  }\n}\n```\n\n## Architecture\n\n### Components\n\n- **`pdfToQuestionsWorkflow`**: Main workflow orchestrating the process\n- **`textQuestionAgent`**: Mastra agent specialized in generating educational questions\n- **`pdfQuestionAgent`**: Complete agent that can handle the full PDF to questions pipeline\n\n### Tools\n\n- **`pdfFetcherTool`**: Downloads PDF files from URLs, extracts text, and generates AI summaries\n- **`generateQuestionsFromTextTool`**: Generates comprehensive questions from summarized content\n\n### Workflow Steps\n\n1. **`download-and-summarize-pdf`**: Downloads PDF from provided URL and generates AI summary\n2. **`generate-questions-from-summary`**: Creates comprehensive questions from the AI summary\n\n## Features\n\n- ‚úÖ **Token Limit Protection**: Demonstrates how to handle large datasets without hitting context limits\n- ‚úÖ **80-95% Token Reduction**: AI summarization drastically reduces processing costs\n- ‚úÖ **Large Context Window**: Uses OpenAI GPT-4.1 Mini to handle large documents efficiently\n- ‚úÖ **Zero System Dependencies**: Pure JavaScript solution\n- ‚úÖ **Single API Setup**: OpenAI for both summarization and question generation\n- ‚úÖ **Fast Text Extraction**: Direct PDF parsing (no OCR needed for text-based PDFs)\n- ‚úÖ **Educational Focus**: Generates focused learning questions from key insights\n- ‚úÖ **Multiple Interfaces**: Workflow, Agent, and individual tools available\n\n## How It Works\n\n### Text Extraction Strategy\n\nThis template uses a **pure JavaScript approach** that works for most PDFs:\n\n1. **Text-based PDFs** (90% of cases): Direct text extraction using `pdf2json`\n   - ‚ö° Fast and reliable\n   - üîß No system dependencies\n   - ‚úÖ Works out of the box\n\n2. **Scanned PDFs**: Would require OCR, but most PDFs today contain embedded text\n\n### Why This Approach?\n\n- **Simplicity**: No GraphicsMagick, ImageMagick, or other system tools needed\n- **Speed**: Direct text extraction is much faster than OCR\n- **Reliability**: Works consistently across different environments\n- **Educational**: Easy for developers to understand and modify\n- **Single Path**: One clear workflow with no complex branching\n\n## Configuration\n\n### Environment Variables\n\n```bash\nOPENAI_API_KEY=your_openai_api_key_here\n```\n\n### Customization\n\nYou can customize the question generation by modifying the `textQuestionAgent`:\n\n```typescript\nexport const textQuestionAgent = new Agent({\n  name: 'Generate questions from text agent',\n  instructions: `\n    You are an expert educational content creator...\n    // Customize instructions here\n  `,\n  model: openai('gpt-4o'),\n});\n```\n\n## Development\n\n### Project Structure\n\n```text\nsrc/mastra/\n‚îú‚îÄ‚îÄ agents/\n‚îÇ   ‚îú‚îÄ‚îÄ pdf-question-agent.ts       # PDF processing and question generation agent\n‚îÇ   ‚îî‚îÄ‚îÄ text-question-agent.ts      # Text to questions generation agent\n‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îú‚îÄ‚îÄ download-pdf-tool.ts         # PDF download tool\n‚îÇ   ‚îú‚îÄ‚îÄ extract-text-from-pdf-tool.ts # PDF text extraction tool\n‚îÇ   ‚îî‚îÄ‚îÄ generate-questions-from-text-tool.ts # Question generation tool\n‚îú‚îÄ‚îÄ workflows/\n‚îÇ   ‚îî‚îÄ‚îÄ generate-questions-from-pdf-workflow.ts # Main workflow\n‚îú‚îÄ‚îÄ lib/\n‚îÇ   ‚îî‚îÄ‚îÄ util.ts                      # Utility functions including PDF text extraction\n‚îî‚îÄ‚îÄ index.ts                         # Mastra configuration\n```\n\n### Testing\n\n```bash\n# Run with a test PDF\nexport OPENAI_API_KEY=\"your-api-key\"\nnpx tsx example.ts\n```\n\n## Common Issues\n\n### \"OPENAI_API_KEY is not set\"\n\n- Make sure you've set the environment variable\n- Check that your API key is valid and has sufficient credits\n\n### \"Failed to download PDF\"\n\n- Verify the PDF URL is accessible and publicly available\n- Check network connectivity\n- Ensure the URL points to a valid PDF file\n- Some servers may require authentication or have restrictions\n\n### \"No text could be extracted\"\n\n- The PDF might be password-protected\n- Very large PDFs might take longer to process\n- Scanned PDFs without embedded text won't work (rare with modern PDFs)\n\n### \"Context length exceeded\" or Token Limit Errors\n\n- **Solution**: Use a smaller PDF file (under ~5-10 pages)\n- **Automatic Truncation**: The tool automatically uses only the first 4000 characters for very large documents\n- **Helpful Errors**: Clear messages guide you to use smaller PDFs when needed\n\n## What Makes This Template Special\n\n### üéØ **True Simplicity**\n\n- Single dependency for PDF processing (`pdf2json`)\n- No system tools or complex setup required\n- Works immediately after `pnpm install`\n- Multiple usage patterns (workflow, agent, tools)\n\n### ‚ö° **Performance**\n\n- Direct text extraction (no image conversion)\n- Much faster than OCR-based approaches\n- Handles reasonably-sized documents efficiently\n\n### üîß **Developer-Friendly**\n\n- Pure JavaScript/TypeScript\n- Easy to understand and modify\n- Clear separation of concerns\n- Simple error handling with helpful messages\n\n### üìö **Educational Value**\n\n- Generates multiple question types\n- Covers different comprehension levels\n- Perfect for creating study materials\n\n## üöÄ Broader Applications\n\nThis token limit protection pattern can be applied to many other scenarios:\n\n### Document Processing\n\n- **Legal documents**: Summarize contracts before analysis\n- **Research papers**: Extract key findings before comparison\n- **Technical manuals**: Create focused summaries for specific topics\n\n### Content Analysis\n\n- **Social media**: Summarize large thread conversations\n- **Customer feedback**: Compress reviews before sentiment analysis\n- **Meeting transcripts**: Extract action items and decisions\n\n### Data Processing\n\n- **Log analysis**: Summarize error patterns before classification\n- **Survey responses**: Compress feedback before theme extraction\n- **Code reviews**: Summarize changes before generating reports\n\n### Implementation Tips\n\n- Use **OpenAI GPT-4.1 Mini** for initial summarization (large context window)\n- Pass **summaries** to downstream tools, not raw data\n- **Chain summaries** for multi-step processing\n- **Preserve metadata** (file size, page count) for context\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request"
}